{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMReMU6gAdtCxTiRU+D8QSg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wilstermanz/holbertonschool-machine_learning/blob/main/supervised_learning/transformer_apps/transformer_apps.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow.compat.v2 as tf\n",
        "\n",
        "pt2en_train = tfds.load('ted_hrlr_translate/pt_to_en',\n",
        "                        split='train',\n",
        "                        as_supervised=True)\n",
        "for pt, en in pt2en_train.take(1):\n",
        "  print(pt.numpy().decode('utf-8'))\n",
        "  print(en.numpy().decode('utf-8'))"
      ],
      "metadata": {
        "id": "7wnobEyL9iZ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 0. Dataset\n",
        "\n",
        "Create the class ```Dataset``` that loads and preps a dataset for machine translation:\n",
        "\n",
        "    Class constructor def __init__(self):\n",
        "        creates the instance attributes:\n",
        "            data_train, which contains the ted_hrlr_translate/pt_to_en tf.data.Dataset train split, loaded as_supervided\n",
        "            data_valid, which contains the ted_hrlr_translate/pt_to_en tf.data.Dataset validate split, loaded as_supervided\n",
        "            tokenizer_pt is the Portuguese tokenizer created from the training set\n",
        "            tokenizer_en is the English tokenizer created from the training set\n",
        "    Create the instance method def tokenize_dataset(self, data): that creates sub-word tokenizers for our dataset:\n",
        "        data is a tf.data.Dataset whose examples are formatted as a tuple (pt, en)\n",
        "            pt is the tf.Tensor containing the Portuguese sentence\n",
        "            en is the tf.Tensor containing the corresponding English sentence\n",
        "        The maximum vocab size should be set to 2**15\n",
        "        Returns: tokenizer_pt, tokenizer_en\n",
        "            tokenizer_pt is the Portuguese tokenizer\n",
        "            tokenizer_en is the English tokenizer\n"
      ],
      "metadata": {
        "id": "mORr9itM9F7Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Dataset:\n",
        "    \"\"\"loads and preps a dataset for machine translation\"\"\"\n",
        "    def __init__(self):\n",
        "        \"\"\"initializes instance of Dataset\"\"\"\n",
        "        self.data_train = tfds.load(name='ted_hrlr_translate/pt_to_en',\n",
        "                                    split='train',\n",
        "                                    as_supervised=True)\n",
        "        self.data_valid = tfds.load(name='ted_hrlr_translate/pt_to_en',\n",
        "                                    split='validation',\n",
        "                                    as_supervised=True)\n",
        "        self.tokenizer_pt, self.tokenizer_en = self.tokenize_dataset(\n",
        "            self.data_train)\n",
        "\n",
        "    def tokenize_dataset(self, data):\n",
        "        \"\"\"\n",
        "        creates sub-word tokenizers for our dataset:\n",
        "\n",
        "        data is a tf.data.Dataset whose examples are formatted as a tuple\n",
        "        (pt, en)\n",
        "            pt is the tf.Tensor containing the Portuguese sentence\n",
        "            en is the tf.Tensor containing the corresponding English sentence\n",
        "        The maximum vocab size should be set to 2**15\n",
        "        Returns: tokenizer_pt, tokenizer_en\n",
        "            tokenizer_pt is the Portuguese tokenizer\n",
        "            tokenizer_en is the English tokenizer\n",
        "        \"\"\"\n",
        "        STE = tfds.deprecated.text.SubwordTextEncoder.build_from_corpus\n",
        "        tokenizer_pt = STE((pt.numpy() for pt, en in data),\n",
        "                           target_vocab_size=2**15)\n",
        "        tokenizer_en = STE((en.numpy() for pt, en in data),\n",
        "                           target_vocab_size=2**15)\n",
        "\n",
        "        return tokenizer_pt, tokenizer_en"
      ],
      "metadata": {
        "id": "GcA42eP_9FcB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = Dataset()\n",
        "for pt, en in data.data_train.take(1):\n",
        "    print(pt.numpy().decode('utf-8'))\n",
        "    print(en.numpy().decode('utf-8'))\n",
        "for pt, en in data.data_valid.take(1):\n",
        "    print(pt.numpy().decode('utf-8'))\n",
        "    print(en.numpy().decode('utf-8'))\n",
        "print(type(data.tokenizer_pt))\n",
        "print(type(data.tokenizer_en))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g8MDD-FG9PpF",
        "outputId": "9039e32f-14e2-4783-d9c6-50a54adcf952"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "e quando melhoramos a procura , tiramos a única vantagem da impressão , que é a serendipidade .\n",
            "and when you improve searchability , you actually take away the one advantage of print , which is serendipity .\n",
            "tinham comido peixe com batatas fritas ?\n",
            "did they eat fish and chips ?\n",
            "<class 'tensorflow_datasets.core.deprecated.text.subword_text_encoder.SubwordTextEncoder'>\n",
            "<class 'tensorflow_datasets.core.deprecated.text.subword_text_encoder.SubwordTextEncoder'>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Encode Tokens\n",
        "\n",
        "Update the class ```Dataset```:\n",
        "\n",
        "    Create the instance method def encode(self, pt, en): that encodes a translation into tokens:\n",
        "        pt is the tf.Tensor containing the Portuguese sentence\n",
        "        en is the tf.Tensor containing the corresponding English sentence\n",
        "        The tokenized sentences should include the start and end of sentence tokens\n",
        "        The start token should be indexed as vocab_size\n",
        "        The end token should be indexed as vocab_size + 1\n",
        "        Returns: pt_tokens, en_tokens\n",
        "            pt_tokens is a np.ndarray containing the Portuguese tokens\n",
        "            en_tokens is a np.ndarray. containing the English tokens\n"
      ],
      "metadata": {
        "id": "BRH7OU3CL5sX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def encode(self, pt, en):\n",
        "    \"\"\"\n",
        "    encodes a translation into tokens:\n",
        "\n",
        "    pt is the tf.Tensor containing the Portuguese sentence\n",
        "    en is the tf.Tensor containing the corresponding English sentence\n",
        "    The tokenized sentences should include the start and end of sentence tokens\n",
        "    The start token should be indexed as vocab_size\n",
        "    The end token should be indexed as vocab_size + 1\n",
        "    Returns: pt_tokens, en_tokens\n",
        "        pt_tokens is a np.ndarray containing the Portuguese tokens\n",
        "        en_tokens is a np.ndarray. containing the English tokens\n",
        "    \"\"\"\n",
        "    pt_start = self.tokenizer_pt.vocab_size\n",
        "    en_start = self.tokenizer_en.vocab_size\n",
        "    pt_tokens = self.tokenizer_pt.encode(pt.numpy())\n",
        "    en_tokens = self.tokenizer_en.encode(en.numpy())\n",
        "\n",
        "    return ([pt_start] + pt_tokens + [pt_start + 1],\n",
        "            [en_start] + en_tokens + [en_start + 1])\n",
        "\n",
        "Dataset.encode = encode"
      ],
      "metadata": {
        "id": "wafD6waJL4tF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = Dataset()\n",
        "for pt, en in data.data_train.take(1):\n",
        "    print(data.encode(pt, en))\n",
        "for pt, en in data.data_valid.take(1):\n",
        "    print(data.encode(pt, en))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aHzV6ZkTMbdD",
        "outputId": "723c2b9f-9053-411c-da8e-9e9a6f05c85e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "([30138, 6, 36, 17925, 13, 3, 3037, 1, 4880, 3, 387, 2832, 18, 18444, 1, 5, 8, 3, 16679, 19460, 739, 2, 30139], [28543, 4, 56, 15, 1266, 20397, 10721, 1, 15, 100, 125, 352, 3, 45, 3066, 6, 8004, 1, 88, 13, 14859, 2, 28544])\n",
            "([30138, 289, 15409, 2591, 19, 20318, 26024, 29997, 28, 30139], [28543, 93, 25, 907, 1366, 4, 5742, 33, 28544])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. TF Encode\n",
        "\n",
        "Update the class ```Dataset```:\n",
        "\n",
        "    Create the instance method def tf_encode(self, pt, en): that acts as a tensorflow wrapper for the encode instance method\n",
        "        Make sure to set the shape of the pt and en return tensors\n",
        "    Update the class constructor def __init__(self):\n",
        "        update the data_train and data_validate attributes by tokenizing the examples\n"
      ],
      "metadata": {
        "id": "6914JwCPPU-V"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def tf_encode(self, pt, en):\n",
        "    \"\"\"\n",
        "    Acts as a tensorflow wrapper for the encode instance method\n",
        "\n",
        "    Make sure to set the shape of the pt and en return tensors\n",
        "    \"\"\"\n",
        "    pt_tokens, en_tokens = tf.py_function(\n",
        "        self.encode, [pt, en], (tf.int64, tf.int64))\n",
        "    pt_tokens.set_shape([None])\n",
        "    en_tokens.set_shape([None])\n",
        "\n",
        "    return pt_tokens, en_tokens\n",
        "\n",
        "Dataset.tf_encode = tf_encode"
      ],
      "metadata": {
        "id": "2FN2DzQHPYrD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def __init__(self):\n",
        "    \"\"\"initializes instance of Dataset\"\"\"\n",
        "    self.data_train = tfds.load(name='ted_hrlr_translate/pt_to_en',\n",
        "                                split='train',\n",
        "                                as_supervised=True)\n",
        "    self.data_valid = tfds.load(name='ted_hrlr_translate/pt_to_en',\n",
        "                                split='validation',\n",
        "                                as_supervised=True)\n",
        "    self.tokenizer_pt, self.tokenizer_en = self.tokenize_dataset(\n",
        "        self.data_train)\n",
        "    self.data_train = self.data_train.map(self.tf_encode)\n",
        "    self.data_valid = self.data_valid.map(self.tf_encode)\n",
        "\n",
        "Dataset.__init__ = __init__"
      ],
      "metadata": {
        "id": "_xB6DLPaQysx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data = Dataset()\n",
        "for pt, en in data.data_train.take(1):\n",
        "    print(pt, en)\n",
        "for pt, en in data.data_valid.take(1):\n",
        "    print(pt, en)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1xNG2GNLQcOg",
        "outputId": "a921d13e-c9fc-4903-d7b1-830f02f8bbe1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[30138     6    36 17925    13     3  3037     1  4880     3   387  2832\n",
            "    18 18444     1     5     8     3 16679 19460   739     2 30139], shape=(23,), dtype=int64) tf.Tensor(\n",
            "[28543     4    56    15  1266 20397 10721     1    15   100   125   352\n",
            "     3    45  3066     6  8004     1    88    13 14859     2 28544], shape=(23,), dtype=int64)\n",
            "tf.Tensor([30138   289 15409  2591    19 20318 26024 29997    28 30139], shape=(10,), dtype=int64) tf.Tensor([28543    93    25   907  1366     4  5742    33 28544], shape=(9,), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Pipeline\n",
        "\n",
        "Update the class ```Dataset``` to set up the data pipeline:\n",
        "\n",
        "    Update the class constructor def __init__(self, batch_size, max_len):\n",
        "        batch_size is the batch size for training/validation\n",
        "        max_len is the maximum number of tokens allowed per example sentence\n",
        "        update the data_train attribute by performing the following actions:\n",
        "            filter out all examples that have either sentence with more than max_len tokens\n",
        "            cache the dataset to increase performance\n",
        "            shuffle the entire dataset\n",
        "            split the dataset into padded batches of size batch_size\n",
        "            prefetch the dataset using tf.data.experimental.AUTOTUNE to increase performance\n",
        "        update the data_validate attribute by performing the following actions:\n",
        "            filter out all examples that have either sentence with more than max_len tokens\n",
        "            split the dataset into padded batches of size batch_size\n"
      ],
      "metadata": {
        "id": "nFE3c1KhcD5k"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def __init__(self, batch_size, max_len):\n",
        "    \"\"\"initializes instance of Dataset\"\"\"\n",
        "    self.data_train = tfds.load(name='ted_hrlr_translate/pt_to_en',\n",
        "                                split='train',\n",
        "                                as_supervised=True)\n",
        "    self.data_valid = tfds.load(name='ted_hrlr_translate/pt_to_en',\n",
        "                                split='validation',\n",
        "                                as_supervised=True)\n",
        "    self.tokenizer_pt, self.tokenizer_en = self.tokenize_dataset(\n",
        "        self.data_train)\n",
        "    self.data_train = self.data_train.map(self.tf_encode)\n",
        "    self.data_valid = self.data_valid.map(self.tf_encode)\n",
        "\n",
        "    def filter_len(pt, en):\n",
        "        \"\"\"Checks length of both parts of a tuple again max_len\"\"\"\n",
        "        return tf.logical_and(tf.size(pt) <= max_len, tf.size(en) <= max_len)\n",
        "\n",
        "    self.data_train = self.data_train.filter(filter_len)\n",
        "    self.data_train = self.data_train.cache()\n",
        "    self.data_train = self.data_train.shuffle(2**15,\n",
        "                                              reshuffle_each_iteration=True)\n",
        "    self.data_train = self.data_train.padded_batch(batch_size)\n",
        "    self.data_train = self.data_train.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "\n",
        "    self.data_valid = self.data_valid.filter(filter_len)\n",
        "    self.data_valid = self.data_valid.padded_batch(batch_size)\n",
        "\n",
        "Dataset.__init__ = __init__"
      ],
      "metadata": {
        "id": "mm8nk5web-U1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.compat.v1.set_random_seed(0)\n",
        "data = Dataset(32, 40)\n",
        "for pt, en in data.data_train.take(1):\n",
        "    print(pt, en)\n",
        "for pt, en in data.data_valid.take(1):\n",
        "    print(pt, en)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NM5ceIOZhFGe",
        "outputId": "ab4aa147-bd20-4f7f-d42c-bec73a82de29"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[30138    21     5 ...     0     0     0]\n",
            " [30138    32    13 ...     0     0     0]\n",
            " [30138     7   880 ...     0     0     0]\n",
            " ...\n",
            " [30138   418  8287 ...     0     0     0]\n",
            " [30138   113  2338 ...     0     0     0]\n",
            " [30138   131    26 ... 17432    34 30139]], shape=(32, 32), dtype=int64) tf.Tensor(\n",
            "[[28543    18   304 ...     0     0     0]\n",
            " [28543    13    17 ...     0     0     0]\n",
            " [28543    17   143 ...     0     0     0]\n",
            " ...\n",
            " [28543    15    30 ...     0     0     0]\n",
            " [28543   192 11962 ...     0     0     0]\n",
            " [28543    59    11 ...    19    71 28544]], shape=(32, 36), dtype=int64)\n",
            "tf.Tensor(\n",
            "[[30138   289 15409 ...     0     0     0]\n",
            " [30138    86   168 ...     0     0     0]\n",
            " [30138  5036     9 ...     0     0     0]\n",
            " ...\n",
            " [30138  1157 29927 ...     0     0     0]\n",
            " [30138    33   837 ...     0     0     0]\n",
            " [30138   126  3308 ...     0     0     0]], shape=(32, 32), dtype=int64) tf.Tensor(\n",
            "[[28543    93    25 ...     0     0     0]\n",
            " [28543    11    20 ...     0     0     0]\n",
            " [28543    11  2850 ...     0     0     0]\n",
            " ...\n",
            " [28543    11   406 ...     0     0     0]\n",
            " [28543     9   152 ...     0     0     0]\n",
            " [28543     4   272 ...     0     0     0]], shape=(32, 35), dtype=int64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 4. Create Masks\n",
        "\n",
        "Create the function ```def create_masks(inputs, target)```: that creates all masks for training/validation:\n",
        "\n",
        "    inputs is a tf.Tensor of shape (batch_size, seq_len_in) that contains the input sentence\n",
        "    target is a tf.Tensor of shape (batch_size, seq_len_out) that contains the target sentence\n",
        "    This function should only use tensorflow operations in order to properly function in the training step\n",
        "    Returns: encoder_mask, combined_mask, decoder_mask\n",
        "        encoder_mask is the tf.Tensor padding mask of shape (batch_size, 1, 1, seq_len_in) to be applied in the encoder\n",
        "        combined_mask is the tf.Tensor of shape (batch_size, 1, seq_len_out, seq_len_out) used in the 1st attention block in the decoder to pad and mask future tokens in the input received by the decoder. It takes the maximum between a look ahead mask and the decoder target padding mask.\n",
        "        decoder_mask is the tf.Tensor padding mask of shape (batch_size, 1, 1, seq_len_in) used in the 2nd attention block in the decoder.\n"
      ],
      "metadata": {
        "id": "kZdQDkYZSxTM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_masks(inputs, target):\n",
        "    \"\"\"\n",
        "    creates all masks for training/validation:\n",
        "\n",
        "    inputs is a tf.Tensor of shape (batch_size, seq_len_in) that contains the\n",
        "    input sentence\n",
        "\n",
        "    target is a tf.Tensor of shape (batch_size, seq_len_out) that contains the\n",
        "    target sentence\n",
        "\n",
        "    This function should only use tensorflow operations in order to properly\n",
        "    function in the training step\n",
        "\n",
        "    Returns: encoder_mask, combined_mask, decoder_mask\n",
        "\n",
        "        encoder_mask is the tf.Tensor padding mask of shape (batch_size, 1, 1,\n",
        "        seq_len_in) to be applied in the encoder\n",
        "\n",
        "        combined_mask is the tf.Tensor of shape (batch_size, 1, seq_len_out,\n",
        "        seq_len_out) used in the 1st attention block in the decoder to pad and\n",
        "        mask future tokens in the input received by the decoder. It takes the\n",
        "        maximum between a look ahead mask and the decoder target padding mask.\n",
        "\n",
        "        decoder_mask is the tf.Tensor padding mask of shape (batch_size, 1, 1,\n",
        "        seq_len_in) used in the 2nd attention block in the decoder.\n",
        "    \"\"\"\n",
        "    def  padding_mask(input):\n",
        "        mask = tf.math.equal(input, 0)\n",
        "        mask = tf.cast(mask, tf.float32)\n",
        "        mask = mask[:, tf.newaxis, tf.newaxis, :]\n",
        "        return mask\n",
        "\n",
        "    def lookahead_mask(seq_len):\n",
        "        mask = 1 - tf.linalg.band_part(tf.fill((seq_len, seq_len), 1.), -1, 0)\n",
        "        return mask\n",
        "\n",
        "    encoder_mask = padding_mask(inputs)\n",
        "    target_mask = padding_mask(target)\n",
        "    combined_mask = tf.maximum(target_mask, lookahead_mask(target.shape[1]))\n",
        "    decoder_mask = padding_mask(inputs)\n",
        "\n",
        "    # This is for testing\n",
        "    print(f'batch_size: {inputs.shape[0]}')\n",
        "    print(f'seq_len_in: {inputs.shape[1]}')\n",
        "    print(f'seq_len_out: {target.shape[1]}\\n')\n",
        "    print(f'encoder mask shape should be: ({inputs.shape[0]}, 1, 1, {inputs.shape[1]})')\n",
        "    print(f'encoder mask shape is:        {encoder_mask.shape}\\n')\n",
        "    print(f'decoder mask shape should be: ({inputs.shape[0]}, 1, 1, {inputs.shape[1]})')\n",
        "    print(f'decoder mask shape is:        {decoder_mask.shape}\\n')\n",
        "    print(f'combined mask shape should be: ({inputs.shape[0]}, 1, {target.shape[1]}, {target.shape[1]})')\n",
        "    print(f'combined mask shape is:        {combined_mask.shape}\\n')\n",
        "    # End tests\n",
        "\n",
        "    return encoder_mask, combined_mask, decoder_mask\n"
      ],
      "metadata": {
        "id": "0x8KoMV-Swy5"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.compat.v1.set_random_seed(0)\n",
        "data = Dataset(32, 40)\n",
        "for inputs, target in data.data_train.take(1):\n",
        "    print(create_masks(inputs, target))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gMy8Z1OCamdp",
        "outputId": "78f8ea12-f522-46f6-8ebd-9e936567fdf2"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "batch_size: 32\n",
            "seq_len_in: 32\n",
            "seq_len_out: 36\n",
            "\n",
            "encoder mask shape should be: (32, 1, 1, 32)\n",
            "encoder mask shape is:        (32, 1, 1, 32)\n",
            "\n",
            "decoder mask shape should be: (32, 1, 1, 32)\n",
            "decoder mask shape is:        (32, 1, 1, 32)\n",
            "\n",
            "combined mask shape should be: (32, 1, 36, 36)\n",
            "combined mask shape is:        (32, 1, 36, 36)\n",
            "\n",
            "(<tf.Tensor: shape=(32, 1, 1, 32), dtype=float32, numpy=\n",
            "array([[[[0., 0., 0., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       ...,\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)>, <tf.Tensor: shape=(32, 1, 36, 36), dtype=float32, numpy=\n",
            "array([[[[0., 1., 1., ..., 1., 1., 1.],\n",
            "         [0., 0., 1., ..., 1., 1., 1.],\n",
            "         [0., 0., 0., ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [0., 0., 0., ..., 1., 1., 1.],\n",
            "         [0., 0., 0., ..., 1., 1., 1.],\n",
            "         [0., 0., 0., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       [[[0., 1., 1., ..., 1., 1., 1.],\n",
            "         [0., 0., 1., ..., 1., 1., 1.],\n",
            "         [0., 0., 0., ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [0., 0., 0., ..., 1., 1., 1.],\n",
            "         [0., 0., 0., ..., 1., 1., 1.],\n",
            "         [0., 0., 0., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       [[[0., 1., 1., ..., 1., 1., 1.],\n",
            "         [0., 0., 1., ..., 1., 1., 1.],\n",
            "         [0., 0., 0., ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [0., 0., 0., ..., 1., 1., 1.],\n",
            "         [0., 0., 0., ..., 1., 1., 1.],\n",
            "         [0., 0., 0., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       ...,\n",
            "\n",
            "\n",
            "       [[[0., 1., 1., ..., 1., 1., 1.],\n",
            "         [0., 0., 1., ..., 1., 1., 1.],\n",
            "         [0., 0., 0., ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [0., 0., 0., ..., 1., 1., 1.],\n",
            "         [0., 0., 0., ..., 1., 1., 1.],\n",
            "         [0., 0., 0., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       [[[0., 1., 1., ..., 1., 1., 1.],\n",
            "         [0., 0., 1., ..., 1., 1., 1.],\n",
            "         [0., 0., 0., ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [0., 0., 0., ..., 1., 1., 1.],\n",
            "         [0., 0., 0., ..., 1., 1., 1.],\n",
            "         [0., 0., 0., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       [[[0., 1., 1., ..., 1., 1., 1.],\n",
            "         [0., 0., 1., ..., 1., 1., 1.],\n",
            "         [0., 0., 0., ..., 1., 1., 1.],\n",
            "         ...,\n",
            "         [0., 0., 0., ..., 0., 1., 1.],\n",
            "         [0., 0., 0., ..., 0., 0., 1.],\n",
            "         [0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)>, <tf.Tensor: shape=(32, 1, 1, 32), dtype=float32, numpy=\n",
            "array([[[[0., 0., 0., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       ...,\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., ..., 1., 1., 1.]]],\n",
            "\n",
            "\n",
            "       [[[0., 0., 0., ..., 0., 0., 0.]]]], dtype=float32)>)\n"
          ]
        }
      ]
    }
  ]
}