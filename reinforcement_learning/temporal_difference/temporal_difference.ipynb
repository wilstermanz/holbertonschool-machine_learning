{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNS5Y5uZEIrb27lz0aGO/9o",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/wilstermanz/holbertonschool-machine_learning/blob/main/reinforcement_learning/temporal_difference/temporal_difference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "44ttzpVrGP1N"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import tensorflow as tf\n",
        "if tf.__version__!='2.11.0':\n",
        "  !pip install tensorflow==2.11.0 --quiet\n",
        "  os.kill(os.getpid(), 9)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gym==0.7.0 --quiet\n",
        "import gym\n",
        "try:\n",
        "    if gym.__version__ != '0.7.0':\n",
        "        !pip uninstall gym --quiet\n",
        "        !pip install gym==0.7.0 --quiet\n",
        "        os.kill(os.getpid(), 9)\n",
        "except Exception:\n",
        "    if gym.version.VERSION != '0.7.0':\n",
        "        !pip uninstall gym --quiet\n",
        "        !pip install gym==0.7.0 --quiet\n",
        "        os.kill(os.getpid(), 9)\n",
        "print(gym.version.VERSION)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RmwWkUiRaiwl",
        "outputId": "7a6ea51a-c2f8-4db1-99c0-1b642a841e3e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.7.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install keras-rl2 --quiet\n",
        "!pip install atari-py --quiet"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ju-EUqXgJiz_",
        "outputId": "53e73d27-9f14-477a-a4bc-48aa6213cc6e"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m52.1/52.1 kB\u001b[0m \u001b[31m915.5 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m540.6/540.6 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for atari-py (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 0. Monte Carlo\n",
        "\n",
        "Write the function ```def monte_carlo(env, V, policy, episodes=5000, max_steps=100, alpha=0.1, gamma=0.99):``` that performs the Monte Carlo algorithm:\n",
        "\n",
        "    env is the openAI environment instance\n",
        "    V is a numpy.ndarray of shape (s,) containing the value estimate\n",
        "    policy is a function that takes in a state and returns the next action to take\n",
        "    episodes is the total number of episodes to train over\n",
        "    max_steps is the maximum number of steps per episode\n",
        "    alpha is the learning rate\n",
        "    gamma is the discount rate\n",
        "    Returns: V, the updated value estimate\n"
      ],
      "metadata": {
        "id": "HYiOb7BRKUTc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def monte_carlo(env, V, policy, episodes=5000, max_steps=100, alpha=0.1, gamma=0.99):\n",
        "\n",
        "    # iterate through episodes\n",
        "    for episode in range(episodes):\n",
        "        cumulative_reward = 0\n",
        "        state = env.reset()\n",
        "        episode_results = []\n",
        "\n",
        "        # perform one episode\n",
        "        for step in range(max_steps):\n",
        "            action = policy(state)\n",
        "            observation, reward, done, info = env.step(action)\n",
        "            episode_results.append([state, reward])\n",
        "            if done:\n",
        "                break\n",
        "            state = observation\n",
        "\n",
        "        # perform Monte Carlo algorithm for episode\n",
        "        episode_results = np.array(episode_results, dtype=int)\n",
        "        for time in range(len(episode_results), 0, -1):\n",
        "            state, reward = episode_results[time - 1]\n",
        "            cumulative_reward = gamma * cumulative_reward + reward\n",
        "            if state not in episode_results[:episode, 0]:\n",
        "                V[state] = V[state] + alpha * (cumulative_reward - V[state])\n",
        "\n",
        "\n",
        "    return V\n"
      ],
      "metadata": {
        "id": "oFdXg3enJrTf"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gym\n",
        "import numpy as np\n",
        "np.random.seed(0)\n",
        "\n",
        "env = gym.make('FrozenLake8x8-v0')\n",
        "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
        "\n",
        "def policy(s):\n",
        "    p = np.random.uniform()\n",
        "    if p > 0.5:\n",
        "        if s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "            return RIGHT\n",
        "        elif s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "            return DOWN\n",
        "        elif s // 8 != 0 and env.desc[s // 8 - 1, s % 8] != b'H':\n",
        "            return UP\n",
        "        else:\n",
        "            return LEFT\n",
        "    else:\n",
        "        if s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "            return DOWN\n",
        "        elif s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "            return RIGHT\n",
        "        elif s % 8 != 0 and env.desc[s // 8, s % 8 - 1] != b'H':\n",
        "            return LEFT\n",
        "        else:\n",
        "            return UP\n",
        "\n",
        "V = np.where(env.desc == b'H', -1, 1).reshape(64).astype('float64')\n",
        "np.set_printoptions(precision=4)\n",
        "env.seed(0)\n",
        "print(monte_carlo(env, V, policy).reshape((8, 8)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x5iJpht3L_VL",
        "outputId": "5548ea8c-9f23-40d0-a042-0d5541bb37c3"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:gym.envs.registration:Making new env: FrozenLake8x8-v0\n",
            "[2023-09-26 14:25:52,559] Making new env: FrozenLake8x8-v0\n",
            "/usr/local/lib/python3.10/dist-packages/gym/envs/registration.py:18: PkgResourcesDeprecationWarning: Parameters to load are deprecated.  Call .resolve and .require separately.\n",
            "  result = entry_point.load(False)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.81    0.9     0.4783  0.4305  0.3874  0.4305  0.6561  0.9   ]\n",
            " [ 0.9     0.729   0.5905  0.4783  0.5905  0.2824  0.2824  0.3874]\n",
            " [ 1.      0.5314  0.729  -1.      1.      0.3874  0.2824  0.4305]\n",
            " [ 1.      0.5905  0.81    0.9     1.     -1.      0.3874  0.6561]\n",
            " [ 1.      0.6561  0.81   -1.      1.      1.      0.729   0.5314]\n",
            " [ 1.     -1.     -1.      1.      1.      1.     -1.      0.9   ]\n",
            " [ 1.     -1.      1.      1.     -1.      1.     -1.      1.    ]\n",
            " [ 1.      1.      1.     -1.      1.      1.      1.      1.    ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. TD(λ)\n",
        "\n",
        "Write the function `def td_lambtha(env, V, policy, lambtha, episodes=5000, max_steps=100, alpha=0.1, gamma=0.99):` that performs the TD(λ) algorithm:\n",
        "\n",
        "    env is the openAI environment instance\n",
        "    V is a numpy.ndarray of shape (s,) containing the value estimate\n",
        "    policy is a function that takes in a state and returns the next action to take\n",
        "    lambtha is the eligibility trace factor\n",
        "    episodes is the total number of episodes to train over\n",
        "    max_steps is the maximum number of steps per episode\n",
        "    alpha is the learning rate\n",
        "    gamma is the discount rate\n",
        "    Returns: V, the updated value estimate\n"
      ],
      "metadata": {
        "id": "lkR7yZ6afd2E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def td_lambtha(env, V, policy, lambtha, episodes=5000, max_steps=100, alpha=0.1, gamma=0.99):\n",
        "    # iterate through episodes\n",
        "    for episode in range(episodes):\n",
        "        state = env.reset()\n",
        "\n",
        "        # e_traces start at zero for the episode\n",
        "        e_trace = np.zeros_like(V)\n",
        "\n",
        "        # perform one episode\n",
        "        for step in range(max_steps):\n",
        "            # act according to policy\n",
        "            action = policy(state)\n",
        "            observation, reward, done, info = env.step(action)\n",
        "\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            # update eligibility\n",
        "            e_trace[state] += 1\n",
        "            e_trace *= lambtha * gamma\n",
        "\n",
        "            # get the td-error and update every state's value estimate\n",
        "            # according to their eligibilities\n",
        "            delta = reward + gamma * V[observation] - V[state]\n",
        "            V += alpha * delta * e_trace\n",
        "\n",
        "            state = observation\n",
        "\n",
        "    return V"
      ],
      "metadata": {
        "id": "dCLl-29Qd68K"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "env = gym.make('FrozenLake8x8-v0')\n",
        "LEFT, DOWN, RIGHT, UP = 0, 1, 2, 3\n",
        "\n",
        "def policy(s):\n",
        "    p = np.random.uniform()\n",
        "    if p > 0.5:\n",
        "        if s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "            return RIGHT\n",
        "        elif s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "            return DOWN\n",
        "        elif s // 8 != 0 and env.desc[s // 8 - 1, s % 8] != b'H':\n",
        "            return UP\n",
        "        else:\n",
        "            return LEFT\n",
        "    else:\n",
        "        if s // 8 != 7 and env.desc[s // 8 + 1, s % 8] != b'H':\n",
        "            return DOWN\n",
        "        elif s % 8 != 7 and env.desc[s // 8, s % 8 + 1] != b'H':\n",
        "            return RIGHT\n",
        "        elif s % 8 != 0 and env.desc[s // 8, s % 8 - 1] != b'H':\n",
        "            return LEFT\n",
        "        else:\n",
        "            return UP\n",
        "\n",
        "V = np.where(env.desc == b'H', -1, 1).reshape(64).astype('float64')\n",
        "np.set_printoptions(precision=4)\n",
        "env.seed(0)\n",
        "print(td_lambtha(env, V, policy, 0.9).reshape((8, 8)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OhXrtHopm4Nc",
        "outputId": "7f1c7aee-17b1-4398-fbe7-d87264b1e37d"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:gym.envs.registration:Making new env: FrozenLake8x8-v0\n",
            "[2023-09-26 14:25:58,647] Making new env: FrozenLake8x8-v0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.6863  0.7055  0.7161  0.7252  0.7357  0.7294  0.7228  0.7185]\n",
            " [ 0.7021  0.7154  0.7303  0.7472  0.7732  0.7525  0.7657  0.7523]\n",
            " [ 0.7242  0.7284  0.7605 -1.      0.8042  0.7763  0.7984  0.7836]\n",
            " [ 0.7524  0.7324  0.7877  0.8485  0.8616 -1.      0.8188  0.8359]\n",
            " [ 0.775   0.7378  0.7653 -1.      0.8792  0.8932  0.8273  0.884 ]\n",
            " [ 0.8604 -1.     -1.      0.9983  0.9032  0.9409 -1.      0.9261]\n",
            " [ 0.8695 -1.      0.9649  0.9922 -1.      0.9832 -1.      0.9433]\n",
            " [ 0.8819  0.9196  0.9298 -1.      1.      0.9867  0.9991  1.    ]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. SARSA(λ)\n",
        "\n",
        "Write the function `def sarsa_lambtha(env, Q, lambtha, episodes=5000, max_steps=100, alpha=0.1, gamma=0.99, epsilon=1, min_epsilon=0.1, epsilon_decay=0.05):` that performs SARSA(λ):\n",
        "\n",
        "    env is the openAI environment instance\n",
        "    Q is a numpy.ndarray of shape (s,a) containing the Q table\n",
        "    lambtha is the eligibility trace factor\n",
        "    episodes is the total number of episodes to train over\n",
        "    max_steps is the maximum number of steps per episode\n",
        "    alpha is the learning rate\n",
        "    gamma is the discount rate\n",
        "    epsilon is the initial threshold for epsilon greedy\n",
        "    min_epsilon is the minimum value that epsilon should decay to\n",
        "    epsilon_decay is the decay rate for updating epsilon between episodes\n",
        "    Returns: Q, the updated Q table\n"
      ],
      "metadata": {
        "id": "QphcDyMJzbEs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def sarsa_lambtha(env, Q, lambtha, episodes=5000, max_steps=100, alpha=0.1,\n",
        "                  gamma=0.99, epsilon=1, min_epsilon=0.1, epsilon_decay=0.05):\n",
        "\n",
        "    def epsilon_greedy(Q, state, epsilon):\n",
        "        # exploration\n",
        "        if np.random.uniform() < epsilon:\n",
        "            return np.random.choice(Q.shape[1])\n",
        "        # exploitation\n",
        "        else:\n",
        "            return np.argmax(Q[state, :])\n",
        "\n",
        "    # play episodes\n",
        "    for episode in range(episodes):\n",
        "\n",
        "        # reset env state at the beginning of each episode\n",
        "        state = env.reset()\n",
        "\n",
        "        # reset eligibility to 0 for each episode\n",
        "        e_trace = np.zeros_like(Q)\n",
        "\n",
        "        # find first action according to policy\n",
        "        action = epsilon_greedy(Q, state, epsilon)\n",
        "\n",
        "        for step in range(max_steps):\n",
        "            # decay epsilon\n",
        "            epsilon = min_epsilon + (1 - min_epsilon) * (\n",
        "                np.exp(-epsilon_decay * episode))\n",
        "\n",
        "            # take first action, observe r, s'\n",
        "            observation, reward, done, info = env.step(action)\n",
        "\n",
        "            # choose a' from s' using policy\n",
        "            next_action = epsilon_greedy(Q, observation, epsilon)\n",
        "\n",
        "            # find delta\n",
        "            delta = reward + (gamma * Q[observation, next_action]) \\\n",
        "                - Q[state, action]\n",
        "\n",
        "            # update eligibility\n",
        "            e_trace[state, action] += 1\n",
        "\n",
        "            # for all s, a:\n",
        "            Q += alpha * delta * e_trace\n",
        "            e_trace *= gamma * lambtha\n",
        "\n",
        "            # check if done\n",
        "            if done:\n",
        "                break\n",
        "\n",
        "            action = next_action\n",
        "            state = observation\n",
        "\n",
        "    return Q"
      ],
      "metadata": {
        "id": "CNi-1rN2zamQ"
      },
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "np.random.seed(0)\n",
        "env = gym.make('FrozenLake8x8-v0')\n",
        "Q = np.random.uniform(size=(64, 4))\n",
        "np.set_printoptions(precision=4)\n",
        "print(sarsa_lambtha(env, Q, 0.9))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cJgFpZNfzwJG",
        "outputId": "74adcbe8-71da-4246-bb72-7d0ce23a42cd"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:gym.envs.registration:Making new env: FrozenLake8x8-v0\n",
            "[2023-09-26 14:54:13,854] Making new env: FrozenLake8x8-v0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0.6751 0.5399 0.5991 0.6146]\n",
            " [0.569  0.5506 0.6948 0.5634]\n",
            " [0.5421 0.6697 0.5076 0.5551]\n",
            " [0.6799 0.563  0.5342 0.5503]\n",
            " [0.5702 0.5168 0.5433 0.562 ]\n",
            " [0.5698 0.5841 0.5518 0.5655]\n",
            " [0.6185 0.6161 0.6514 0.6225]\n",
            " [0.6692 0.6142 0.6416 0.6112]\n",
            " [0.598  0.6946 0.608  0.5809]\n",
            " [0.7055 0.6087 0.6112 0.5702]\n",
            " [0.682  0.5616 0.6079 0.5397]\n",
            " [0.3986 0.4996 0.4892 0.678 ]\n",
            " [0.5955 0.5288 0.5465 0.5586]\n",
            " [0.6176 0.6244 0.6354 0.6411]\n",
            " [0.7083 0.6466 0.6574 0.6469]\n",
            " [0.6182 0.6071 0.6169 0.6584]\n",
            " [0.6566 0.7563 0.6364 0.6397]\n",
            " [0.5981 0.7691 0.6528 0.5993]\n",
            " [0.7591 0.5113 0.4666 0.5392]\n",
            " [0.2828 0.1202 0.2961 0.1187]\n",
            " [0.5112 0.5743 0.4602 0.497 ]\n",
            " [0.6011 0.6412 0.7813 0.6693]\n",
            " [0.7781 0.6965 0.717  0.7144]\n",
            " [0.6717 0.7463 0.5937 0.6758]\n",
            " [0.681  0.6884 0.8111 0.689 ]\n",
            " [0.5859 0.8456 0.716  0.7061]\n",
            " [0.7085 0.8558 0.6916 0.6272]\n",
            " [0.6685 0.8575 0.6714 0.5628]\n",
            " [0.6877 0.8238 0.7181 0.6877]\n",
            " [0.8811 0.5813 0.8817 0.6925]\n",
            " [0.7418 0.8189 0.7575 0.7514]\n",
            " [0.7623 0.7458 0.6196 0.7127]\n",
            " [0.633  0.678  0.8443 0.6936]\n",
            " [0.6481 0.8907 0.6611 0.6936]\n",
            " [0.7129 0.7499 0.7348 0.8631]\n",
            " [0.8965 0.3676 0.4359 0.8919]\n",
            " [0.822  0.758  0.3883 0.7513]\n",
            " [0.7875 0.7436 0.4169 0.8355]\n",
            " [0.7044 0.8617 0.461  0.7667]\n",
            " [0.7882 0.958  0.6868 0.448 ]\n",
            " [0.581  0.6305 0.5876 0.8768]\n",
            " [0.9755 0.8558 0.0117 0.36  ]\n",
            " [0.73   0.1716 0.521  0.0543]\n",
            " [0.1854 0.0185 0.8354 0.2738]\n",
            " [0.3454 0.8586 0.7044 0.0318]\n",
            " [0.1647 0.6031 0.7642 0.2379]\n",
            " [0.9342 0.614  0.5356 0.5899]\n",
            " [1.0684 0.5536 0.5692 0.3656]\n",
            " [0.278  0.4408 0.4858 0.4093]\n",
            " [0.2274 0.2544 0.058  0.4344]\n",
            " [0.3118 0.5543 0.3784 0.2047]\n",
            " [0.0247 0.1056 0.7083 0.4537]\n",
            " [0.5366 0.8967 0.9903 0.2169]\n",
            " [0.6631 0.2633 0.0207 0.7864]\n",
            " [0.32   0.3835 0.5883 0.831 ]\n",
            " [0.6484 1.2738 0.7522 0.8021]\n",
            " [0.2879 0.4015 0.4603 0.3855]\n",
            " [0.4493 0.4626 0.2979 0.2519]\n",
            " [0.4525 0.1059 0.2489 0.443 ]\n",
            " [0.3742 0.4636 0.2776 0.5868]\n",
            " [0.8639 0.1175 0.5174 0.1321]\n",
            " [0.7169 0.3961 0.5654 0.1833]\n",
            " [0.1448 0.4881 0.3556 0.9404]\n",
            " [0.7653 0.7487 0.9037 0.0834]]\n"
          ]
        }
      ]
    }
  ]
}